{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from benchmark import Engine\n",
    "from data import Benchmark\n",
    "from llms import OpenAIHelper\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))  # one level up\n",
    "\n",
    "config_path = os.path.join(project_root, \"config.json\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_methods = [\n",
    "    \"baseline\",\n",
    "    \"Authoritative\",\n",
    "    \"Statistics\",\n",
    "    \"Citations\",\n",
    "    \"Fluency\",\n",
    "    \"UniqueWords\",\n",
    "    \"TechnicalTerms\",\n",
    "    \"SimpleLanguage\",\n",
    "    \"Quotes\",\n",
    "    \"LLMstxt\",\n",
    "    \"ContentImprovement\",\n",
    "    \"seo_baseline-1\"  # only 1 document uses the method. The document is pushed to position 1 (you can change 1 to any position)\n",
    "    \"seo_baseline_game_theory\",  # it pushes the documents in selected_docs.json to the top positions. If there is only one document, the method is equivalent to \"seo_baseline-1\"\n",
    "]\n",
    "list_splits = [\"retail\", \"videogames\", \"news\", \"web\", \"debate\", \"books\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"parameterlab/c-seo-bench\"\n",
    "llm_name = \"gpt-4o-mini-2024-07-18\"\n",
    "llm = OpenAIHelper(llm_name)\n",
    "num_docs_in_context = 10\n",
    "sample_size = None\n",
    "method = list_methods[1]  # choose the method to use\n",
    "split = list_splits[-2]  # choose the split to use\n",
    "\n",
    "# check if the selected_docs.json file exists\n",
    "if not os.path.exists(os.path.join(project_root, \"data\", split, \"selected_docs.json\")):\n",
    "    raise FileNotFoundError(\n",
    "        \"The selected_docs.json file does not exist. Please run steps 1 and 2 to select the documents and improve them with a C-SEO method. Only then you can run C-SEO Benchmark (step 3).\"\n",
    "    )\n",
    "print(\n",
    "    f\"You will run a conversational search engine powered by {llm_name} on the {split} split after improving the documents with the {method} C-SEO method.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_developer_prompt(split):\n",
    "    if split in [\"web\", \"news\", \"debate\"]:\n",
    "        developer_prompt = \"Write an accurate and concise answer for the given user question, using _only_ the provided summarized web search results. The answer should be correct, high-quality, and written by an expert using an unbiased and journalistic tone. The answer should be informative, interesting, and engaging. The answer's logic and reasoning should be rigorous and defensible. Every sentence in the answer should be _immediately followed_ by an in-line citation to the search result(s). The cited search result(s) should fully support _all_ the information in the sentence. Search results need to be cited using [index]. When citing several search results, use [1][2][3] format rather than [1, 2, 3]. You can use multiple search results to respond comprehensively while avoiding irrelevant search results. Search results are not sorted.\"\n",
    "    elif split == \"videogames\":\n",
    "        developer_prompt = \"You are a videogame recommender system for an online marketplace. Write an accurate and concise answer for the given user question, using _only_ the provided summarized web search results. The answer should be correct, high-quality, and written by an expert using an unbiased and journalistic tone. The answer should be informative, interesting, and engaging. The answer's logic and reasoning should be rigorous and defensible. Every sentence in the answer should be _immediately followed_ by an in-line citation to the search result(s). The cited search result(s) should fully support _all_ the information in the sentence. Search results need to be cited using [index]. When citing several search results, use [1][2][3] format rather than [1, 2, 3]. You can use multiple search results to respond comprehensively while avoiding irrelevant search results. Search results are not sorted.\"\n",
    "    elif split == \"retail\":\n",
    "        developer_prompt = \"Your are a product recommender system for an online marketplace. Write an accurate and concise answer for the given user question, using _only_ the provided summarized web search results. The answer should be correct, high-quality, and written by an expert using an unbiased and journalistic tone. The answer should be informative, interesting, and engaging. The answer's logic and reasoning should be rigorous and defensible. Every sentence in the answer should be _immediately followed_ by an in-line citation to the search result(s). The cited search result(s) should fully support _all_ the information in the sentence. Search results need to be cited using [index]. When citing several search results, use [1][2][3] format rather than [1, 2, 3]. You can use multiple search results to respond comprehensively while avoiding irrelevant search results. Search results are not sorted.\"\n",
    "    elif split == \"books\":\n",
    "        developer_prompt = \"Your are a book recommender system for an online marketplace. Write an accurate and concise answer for the given user question, using _only_ the provided summarized web search results. The answer should be correct, high-quality, and written by an expert using an unbiased and journalistic tone. The answer should be informative, interesting, and engaging. The answer's logic and reasoning should be rigorous and defensible. Every sentence in the answer should be _immediately followed_ by an in-line citation to the search result(s). The cited search result(s) should fully support _all_ the information in the sentence. Search results need to be cited using [index]. When citing several search results, use [1][2][3] format rather than [1, 2, 3]. You can use multiple search results to respond comprehensively while avoiding irrelevant search results. Search results are not sorted.\"\n",
    "    else:\n",
    "        raise ValueError(f\"Split {split} not supported.\")\n",
    "    return developer_prompt\n",
    "\n",
    "\n",
    "doc_type_mapping = {\n",
    "    \"books\": \"Synopsis\",\n",
    "    \"web\": \"Web Page Snippet\",\n",
    "    \"debate\": \"Web Page Snippet\",\n",
    "    \"news\": \"News Article\",\n",
    "    \"retail\": \"Product Description\",\n",
    "    \"videogames\": \"Game Description\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = Engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "dataset = Benchmark(\n",
    "    num_docs_in_context=num_docs_in_context,\n",
    "    method=method,\n",
    "    sample_size=sample_size,\n",
    "    data_path=dataset_path,\n",
    "    split=split,\n",
    "    doc_type=doc_type_mapping[split],\n",
    "    selected_documents_path=os.path.join(\n",
    "        project_root, \"data\", split, \"selected_docs.json\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[1][\"user_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the benchmark\n",
    "running_folder = os.path.join(project_root, \"experiments\", \"running\", split, method)\n",
    "batch_id = engine.run_benchmark(\n",
    "    dataset, get_developer_prompt(split), llm, running_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = llm.get_status(batch_id)\n",
    "print(f\"Status for {split} x {method}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the status is \"completed\", we can download and process the results\n",
    "\n",
    "results_folder = running_folder.replace(\"running\", \"results\")\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "results, cost = llm.retrieve_results(batch_id)\n",
    "df = engine.process_benchmark_responses(results, results_folder)\n",
    "df.to_parquet(\n",
    "    os.path.join(results_folder, \"responses.parquet\"),\n",
    "    index=False,\n",
    ")\n",
    "print(f\"Results saved in {results_folder} -- Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cseo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
