{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b302a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef319104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13eac822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_significant_improvements(df_baseline, df_method, max_citations=5):\n",
    "    \"\"\"\n",
    "    Checks whether pages in the 'Boost Product Index' have significantly lower\n",
    "    ranks in df_method's 'Citation Order' compared to df_baseline.\n",
    "\n",
    "    The function:\n",
    "    1. Iterates over both DataFrames row by row.\n",
    "    2. Gathers the ranks of items that appear in each row's 'Boost Product Index'.\n",
    "       For each boosted item, we find its position in the baseline and method ranks.\n",
    "    3. Computes a difference: baseline_position - method_position.\n",
    "       (If this difference is positive, it means that under the method, the item\n",
    "        appears at a lower index, i.e. a better/lower rank.)\n",
    "    4. Applies the Wilcoxon signed-rank test to see if the differences are\n",
    "       significantly > 0.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing:\n",
    "            - 'statistic': The Wilcoxon test statistic.\n",
    "            - 'pvalue': The Wilcoxon test p-value.\n",
    "            - 'mean_diff': The average of the rank differences.\n",
    "            - 'count': How many boosted items were analyzed in total.\n",
    "    \"\"\"\n",
    "\n",
    "    all_differences = []\n",
    "    n = min(len(df_baseline), len(df_method))\n",
    "    # reset index\n",
    "    df_baseline = df_baseline.reset_index(drop=True)\n",
    "    df_method = df_method.reset_index(drop=True)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Ensure data is a Python list so we can use .index()\n",
    "        baseline_order = df_baseline.loc[i, \"Citation Order\"]\n",
    "        method_order = df_method.loc[i, \"Citation Order\"]\n",
    "\n",
    "        if isinstance(baseline_order, np.ndarray):\n",
    "            baseline_order = baseline_order.tolist()[:max_citations]\n",
    "        if isinstance(method_order, np.ndarray):\n",
    "            method_order = method_order.tolist()[:max_citations]\n",
    "\n",
    "        boosted_items = df_method.loc[i, \"Boost Product Index\"]\n",
    "        # if boosted item is not a list (for compatibility with older results)\n",
    "        if not isinstance(boosted_items, list):\n",
    "            boosted_items = [boosted_items]\n",
    "        # For each boosted item, compute the rank difference (baseline - method)\n",
    "        for item in boosted_items:\n",
    "            if (item in baseline_order) and (item in method_order):\n",
    "                diff = baseline_order.index(item) - method_order.index(item)\n",
    "                all_differences.append(diff)\n",
    "            elif (item in baseline_order) and (item not in method_order):\n",
    "                # rank after = max_citations\n",
    "                diff = baseline_order.index(item) - len(method_order)\n",
    "                all_differences.append(diff)\n",
    "            elif (item not in baseline_order) and (item in method_order):\n",
    "                # rank before = max_citations\n",
    "                diff = len(baseline_order) - method_order.index(item)\n",
    "                all_differences.append(diff)\n",
    "            else:\n",
    "                all_differences.append(0)\n",
    "\n",
    "    if len(all_differences) == 0:\n",
    "        # print(\"No boosted items found in the data, so no comparison could be made.\")\n",
    "        return {\"statistic\": None, \"pvalue\": None, \"mean_diff\": None, \"count\": 0}\n",
    "\n",
    "    # We want to test if the difference is significantly > 0\n",
    "    stat, pvalue = wilcoxon(all_differences, alternative=\"greater\")\n",
    "    mean_diff = np.mean(all_differences)\n",
    "    std_diff = np.std(all_differences)\n",
    "    # Print a human-friendly explanation\n",
    "    # print(\"Wilcoxon test results:\")\n",
    "    # print(f\"Statistic: {stat:.4f}\")\n",
    "    # print(f\"P-value: {pvalue:.4g}\")\n",
    "    # print(f\"Mean difference (baseline - method): {mean_diff:.4f}\")\n",
    "\n",
    "    # if pvalue is not None and pvalue < 0.05:\n",
    "    #     print(\n",
    "    #         \"Conclusion: There is a significant improvement for boosted terms (p < 0.05).\"\n",
    "    #     )\n",
    "    # else:\n",
    "    #     print(\n",
    "    #         \"Conclusion: No significant improvement found for boosted terms (p >= 0.05).\"\n",
    "    #     )\n",
    "    # how many delta rank > 0\n",
    "    # print(\n",
    "    #     f\"Number of boosted items analyzed: {len(all_differences)}. \"\n",
    "    #     f\"Number of significant improvements: {Counter(np.array(all_differences) > 0)[True]}\"\n",
    "    #     f\"Number of significant deteriorations: {Counter(np.array(all_differences) < 0)[True]}\"\n",
    "    #     f\"Number of no change: {Counter(np.array(all_differences) == 0)[True]}\"\n",
    "    # )\n",
    "\n",
    "    return {\n",
    "        \"statistic\": stat,\n",
    "        \"pvalue\": pvalue,\n",
    "        \"Delta Rank\": (mean_diff, std_diff),\n",
    "        \"diffs\": all_differences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1afb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seo_baseline_improvements(\n",
    "    df_baseline, df_method, new_position, max_citations=5\n",
    "):\n",
    "    \"\"\"\n",
    "    In the seo baseline, the index to boost is the first one.\n",
    "    This means that if index to boost i 3, the doc #3 in the baseline is used as #0 in df_method\n",
    "    new_position is 1-based\n",
    "    \"\"\"\n",
    "\n",
    "    all_differences = []\n",
    "    n = min(len(df_baseline), len(df_method))\n",
    "    # reset index\n",
    "    df_baseline = df_baseline.reset_index(drop=True)\n",
    "    df_method = df_method.reset_index(drop=True)\n",
    "    new_position = new_position - 1  # convert to 0-based index\n",
    "\n",
    "    for i in range(n):\n",
    "        # Ensure data is a Python list so we can use .index()\n",
    "        baseline_order = df_baseline.loc[i, \"Citation Order\"]\n",
    "        method_order = df_method.loc[i, \"Citation Order\"]\n",
    "\n",
    "        if isinstance(baseline_order, np.ndarray):\n",
    "            baseline_order = baseline_order.tolist()[:max_citations]\n",
    "        if isinstance(method_order, np.ndarray):\n",
    "            method_order = method_order.tolist()[:max_citations]\n",
    "\n",
    "        boosted_items = df_method.loc[i, \"Boost Product Index\"]\n",
    "        # if boosted item is not a list (for compatibility with older results)\n",
    "        if not isinstance(boosted_items, list):\n",
    "            boosted_items = [boosted_items]\n",
    "\n",
    "        # I need to calculate the difference between rank of item 0 in method_order and rank of item in baseline_order\n",
    "\n",
    "        # For each boosted item, compute the rank difference (baseline - method)\n",
    "        for item in boosted_items:\n",
    "            if (item in baseline_order) and (new_position in method_order):\n",
    "                diff = baseline_order.index(item) - method_order.index(new_position)\n",
    "                all_differences.append(diff)\n",
    "            elif (item in baseline_order) and (new_position not in method_order):\n",
    "                # rank after = max_citations\n",
    "                diff = baseline_order.index(item) - len(method_order)\n",
    "                all_differences.append(diff)\n",
    "            elif (item not in baseline_order) and (new_position in method_order):\n",
    "                # rank before = max_citations\n",
    "                diff = len(baseline_order) - method_order.index(new_position)\n",
    "                all_differences.append(diff)\n",
    "            else:\n",
    "                all_differences.append(0)\n",
    "\n",
    "    if len(all_differences) == 0:\n",
    "        # print(\"No boosted items found in the data, so no comparison could be made.\")\n",
    "        return {\"statistic\": None, \"pvalue\": None, \"mean_diff\": None, \"count\": 0}\n",
    "\n",
    "    # We want to test if the difference is significantly > 0\n",
    "    stat, pvalue = wilcoxon(all_differences, alternative=\"greater\")\n",
    "    mean_diff = np.mean(all_differences)\n",
    "    std_diff = np.std(all_differences)\n",
    "    # Print a human-friendly explanation\n",
    "    # print(\"Wilcoxon test results:\")\n",
    "    # print(f\"Statistic: {stat:.4f}\")\n",
    "    # print(f\"P-value: {pvalue:.4g}\")\n",
    "    # print(f\"Mean difference (baseline - method): {mean_diff:.4f}\")\n",
    "\n",
    "    # if pvalue is not None and pvalue < 0.05:\n",
    "    #     print(\n",
    "    #         \"Conclusion: There is a significant improvement for boosted terms (p < 0.05).\"\n",
    "    #     )\n",
    "    # else:\n",
    "    #     print(\n",
    "    #         \"Conclusion: No significant improvement found for boosted terms (p >= 0.05).\"\n",
    "    #     )\n",
    "\n",
    "    print(\n",
    "        f\"Number of boosted items analyzed: {len(all_differences)}. \"\n",
    "        f\"Number of significant improvements: {Counter(np.array(all_differences) > 0)[True]}\"\n",
    "        f\"Number of significant deteriorations: {Counter(np.array(all_differences) < 0)[True]}\"\n",
    "        f\"Number of no change: {Counter(np.array(all_differences) == 0)[True]}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"statistic\": stat,\n",
    "        \"pvalue\": pvalue,\n",
    "        \"Delta Rank\": (mean_diff, std_diff),\n",
    "        \"diffs\": all_differences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cffc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonferroni_holm_correction(df_pvalues):\n",
    "    # Make a clean copy and set Method as the index\n",
    "    df_corrected = df_pvalues.set_index(\"Method\").copy()\n",
    "\n",
    "    # Apply Holm-Bonferroni correction to each dataset column\n",
    "    for col in df_corrected.columns:\n",
    "        # Convert p-values to float just in case\n",
    "        pvals = df_corrected[col].astype(float).values\n",
    "        _, pvals_corrected, _, _ = multipletests(pvals, method=\"holm\")\n",
    "        df_corrected[col] = pvals_corrected\n",
    "\n",
    "    # Optional: round for display\n",
    "    df_corrected = df_corrected.round(6)\n",
    "\n",
    "    # Show the corrected DataFrame\n",
    "    return df_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f2a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"retail\"\n",
    "cseo_method = \"LLMGuidance\"\n",
    "llm_name = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5290a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = os.path.join(\n",
    "    project_root,\n",
    "    \"experiments/results\",\n",
    "    dataset,\n",
    "    \"Original\",\n",
    "    llm_name,\n",
    "    \"AdoptionMode.NONE\",\n",
    ")\n",
    "df_baseline = pd.read_parquet(os.path.join(baseline_path, \"responses.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b3ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_path = os.path.join(\n",
    "    project_root,\n",
    "    \"experiments/results\",\n",
    "    dataset,\n",
    "    cseo_method,\n",
    "    llm_name,\n",
    "    \"AdoptionMode.UNILATERAL/\",\n",
    ")\n",
    "df_method = pd.read_parquet(os.path.join(method_path, \"responses.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47dde55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_significant_improvements(df_baseline, df_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4effc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference (baseline - method): 0.3640 +/- 1.4709\n",
      "Wilcoxon test statistic: 13667.0\n",
      "Wilcoxon test p-value (before Bonferroni-Holm correction): 6.65020377670196e-08\n"
     ]
    }
   ],
   "source": [
    "results[\"Delta Rank\"]\n",
    "print(\n",
    "    f\"Mean difference (baseline - method): {results['Delta Rank'][0]:.4f} +/- {results['Delta Rank'][1]:.4f}\"\n",
    ")\n",
    "print(f\"Wilcoxon test statistic: {results['statistic']}\")\n",
    "print(f\"Wilcoxon test p-value (before Bonferroni-Holm correction): {results['pvalue']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cseo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
